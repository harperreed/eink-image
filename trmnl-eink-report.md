# Rendering Photographs on a Monochrome E-Ink Display (TRMNL) with Rust

## Introduction

Displaying full-color JPEG photographs on a **monochrome e-ink** screen is challenging. The TRMNL device features a 7.5" **1-bit EPD (electronic paper display)**, meaning each pixel can only be black or white. To render photos accurately under these constraints, images must be carefully transformed. This report discusses the challenges of showing photographic content on a black-and-white e-ink, outlines preprocessing steps (scaling, grayscale conversion, contrast adjustment), explores **dithering** methods (like Floyd–Steinberg and ordered dithering) optimized for e-ink, addresses tonal compression and dynamic range, and considers performance implications on an embedded Rust (no_std) environment. We also recommend Rust libraries and techniques suitable for constrained systems.

## Challenges of Monochrome E-Ink Photo Display

**Limited Color Depth:** A monochrome e-ink display only has two colors (black and white), unlike the 24-bit color of typical photos. This extreme reduction in color depth requires techniques to **simulate intermediate shades**. Without special processing, a photograph would appear as a high-contrast silhouette or lose most detail. We must use **dithering** (strategic pixel patterns) to create the _illusion_ of gray shades. This was a known challenge even on early 1-bit computer displays, where rendering photos was considered “something of an art form”.

**Contrast and Dynamic Range:** E-ink screens have a limited reflectance range – “white” is actually light gray and “black” is dark gray compared to emissive displays. Photographic images often have subtle tonal gradations that can be lost when forced into pure black or white. High dynamic range scenes risk blown-out highlights (rendered as all-white) or crushed shadows (all-black). Careful tone mapping is needed so important details aren’t lost due to the display’s contrast limits.

**Resolution and Detail:** The TRMNL’s resolution (≈800×480 pixels for 7.5″) means photos must be scaled down, potentially losing fine detail. Small features may vanish or cause moiré if not resampled properly. Additionally, the **sharpness** of e-ink (which has high native contrast for edges) can make dithering patterns or pixelation more apparent. Balancing detail preservation with smoothing is tricky.

**Refresh Characteristics:** E-Ink displays refresh slowly (full refresh on a 7.5″ EPD can take \~1-2 seconds) and can exhibit **ghosting** (faint remnants of the previous image). While ghosting is mitigated by doing a full white/black flash before drawing a new photo, it means we cannot rapidly update photos. This isn’t a concern for a static image, but it underscores that any image transformation should produce a final static bitmap that can be flashed to the display in one go. The slow refresh also means any preprocessing should ideally be done **before** the refresh cycle to avoid keeping the display in transition longer than necessary.

**Embedded Processing Constraints:** In a TRMNL context (battery-powered, embedded controller), we likely have limited CPU and memory. Complex image processing (JPEG decoding, scaling, and dithering) must be efficient. Floating-point operations might be slow or unavailable (many microcontrollers lack an FPU), and memory for full images is limited. We must often favor integer math, fixed-point arithmetic, and streaming algorithms. In summary, the device environment demands an algorithm that is not only effective but also lightweight in terms of computation and memory usage.

## Image Preprocessing for E-Ink

Before dithering and display, the source JPEG photo should be preprocessed:

1. **Decode and Convert to Grayscale:** Use a JPEG decoder that can run in an embedded context to obtain the image pixels. Convert the image to grayscale, since color distinctions will be lost on a monochrome screen. A perceptual luminance formula (e.g., `Y = 0.299*R + 0.587*G + 0.114*B`) can produce a single brightness value for each pixel. This conversion may need to account for gamma (more on this in _Tonal Compression_ below). Rust crates like _zune-jpeg_ or _jpeg-decoder_ can handle JPEG decoding in pure Rust; _zune-jpeg_, for example, supports grayscale output and **no_std** operation (with an allocator). If memory is tight, consider decoding the JPEG at a reduced scale or streaming the decode to avoid holding the entire image in RAM.

2. **Scaling to Display Resolution:** Resize the image to the e-ink’s native resolution (for TRMNL, 800×480 or similar). Use a high-quality downscaling filter (bilinear or bicubic interpolation) to preserve important details and avoid aliasing. In constrained environments, bilinear interpolation is a good compromise between quality and complexity. If the image aspect ratio differs from the screen, decide whether to crop or letterbox/pillarbox the image. Cropping can fill the screen but may cut off content, while letterboxing leaves borders. The scaling should ensure the main subject fits well within 800×480. Some JPEG decoders allow downsampling by powers of 2 in the decode stage, which can accelerate scaling for large images. Otherwise, a manual downscale loop can be implemented in Rust. (If offline processing is an option, using a tool like ImageMagick to pre-size images is even simpler.)

3. **Contrast and Brightness Adjustment:** To optimize the limited dynamic range, it often helps to adjust the image’s global contrast **before** dithering. Increasing contrast can make the photo punchier on e-ink, but too much contrast will push midtones into pure black or white, reducing detail. Conversely, a mild brightness boost might prevent midtones from all turning black. A useful technique is to apply a **gamma correction**: for example, if the source is in sRGB (gamma ≈2.2), consider converting to linear luminance for processing, then apply an appropriate gamma curve back for the display. This ensures that the dithering distributes error according to human perception. In fact, experts note that performing dithering in linear light yields more accurate results – using gamma-corrected values directly can make the image “too dark” on average. In practice, this means you might linearize the grayscale (raise pixel values to 2.2, or use a lookup table) before applying the thresholding/dither step, so that the average of black and white pixels truly appears as a mid gray. Any global tonal adjustments (like a slight contrast stretch or using a histogram equalization for extreme cases) should be done carefully; subtle changes can help utilize the full 1-bit range (ensuring some pixels hit true black and some true white for contrast) without losing important content in pure tones.

4. **Optional Filtering:** Depending on the photo, additional preprocessing can help. For example, a mild sharpening filter _before_ dithering can enhance edges so they survive the loss of resolution. Conversely, a noise reduction or smoothing can prevent random noise in the source from turning into distracting dither patterns. Any such filters must be tuned – the goal is to maximize clarity on the final 1-bit image. In many cases, simple is better, and the primary adjustments are resizing and contrast. You can iterate by simulating the final 1-bit output on a PC (for example, run the photo through ImageMagick with `-monochrome` to see how it will look) and adjust accordingly.

With the image in a grayscale, contrast-optimized, 800×480 buffer, we can proceed to dithering, which is the key step for rendering on a 1-bit display.

## Dithering Strategies for Monochrome E-Ink

Dithering is essential for representing continuous-tone images with only black and white pixels. It works by creating patterns of black and white that average out to the desired gray level over an area. There are two major classes of dithering relevant to e-ink:

### Error Diffusion Dithering (Floyd–Steinberg)

**Floyd–Steinberg (FS)** is a popular error diffusion algorithm that produces very good photographic results. It scans through the image pixels and at each pixel decides to output either black or white. It then computes the _error_ (the difference between the original gray value and the binary output) and propagates that error to neighboring pixels that have not yet been processed. This way, the algorithm spreads the “missing” brightness into nearby pixels. The classic Floyd–Steinberg kernel diffuses the error to four neighbors: 7/16 to the pixel on the right, 5/16 to the one directly below, 3/16 to the below-left, and 1/16 to below-right. The result is an image with fine-grained noise that, from a distance, simulates the original shades remarkably well.

Floyd–Steinberg dithering is _especially suitable for e-paper displays_ to show rich gradation with a limited palette. It preserves detail and tonal variations better than simpler methods. For example, if a region of the photo is 50% gray, FS dithering will sprinkle black and white pixels such that roughly half are black, producing an optical 50% gray. Edges and textures in the photo tend to be represented with scattered pixels that maintain visual detail.

**Optimizing FS for E-Ink:** One consideration is that a raw Floyd–Steinberg dither at full strength can result in a very “noisy” image – fine speckles everywhere – which on a reflective e-ink might look grainy. On the other hand, reducing the diffusion can make the image more contrasty but risk losing detail in highlights or shadows. A tip from ImageMagick users is to adjust the _diffusion amount_ less than 100%. For instance, using only \~80% of the error for diffusion yields a pleasing balance where lighter areas don’t get too many black specks (preventing them from looking dirty or “snowy”). At lower diffusion (e.g. 60%), _highlights blow out_ to pure white – meaning you lose subtle light grays entirely. At 100% diffusion, you preserve all gradients (smooth transitions) but the whole image has a heavy dither noise. Finding a sweet spot (often around 75–85% diffusion) can keep lighter areas cleaner (whiter) while still shading smoothly in mid-tones. This technique effectively **compresses the tonal range** slightly: very bright regions stay white, sacrificing some detail there to avoid pepper noise, whereas mid-tones and shadows still get diffused detail. On an e-ink display, this often improves perceived contrast and readability of the photo.

Another tweak for error diffusion on e-ink is to ensure a **full white/black refresh** is done before displaying the dithered image. FS will produce isolated black pixels on white background (and vice versa); a full refresh (with the typical white flash) will make those crisp. Partial refresh modes sometimes do not completely flip isolated pixels due to voltage waveforms optimized for text, so a full update avoids any faint pixels. In code, implementing Floyd–Steinberg is straightforward – the pseudocode below demonstrates the idea:

```rust
// Assume gray[x][y] is 0..255, output[x][y] initially 0.
for y in 0..height {
    for x in 0..width {
        let old_val = gray[y][x];
        // Decide output pixel
        let new_val = if old_val >= 128 { 255 } else { 0 };
        output[y][x] = new_val;
        // Compute quantization error (0..255 range)
        let err = old_val as i16 - new_val as i16;
        // Distribute the error to neighbors (check bounds)
        if x+1 < width {
            gray[y][x+1] = clamp(gray[y][x+1] as i16 + err * 7 / 16);
        }
        if y+1 < height {
            if x > 0 {
                gray[y+1][x-1] = clamp(gray[y+1][x-1] as i16 + err * 3 / 16);
            }
            gray[y+1][x] = clamp(gray[y+1][x] as i16 + err * 5 / 16);
            if x+1 < width {
                gray[y+1][x+1] = clamp(gray[y+1][x+1] as i16 + err * 1 / 16);
            }
        }
    }
}
```

_(In an embedded context, use integer math for speed. The `clamp()` ensures pixel values stay in 0..255 range. This loop can be implemented without floating-point, as shown, which is important for no_std environments.)_

Floyd–Steinberg’s strength is preserving _gradients and details_. Its weakness is performance (though on a 800×480 image, \~384k pixels, it’s usually fine even on microcontrollers with appropriate optimizations) and the fact it introduces a random-looking noise pattern. But for photographs on e-ink, the consensus is that error diffusion dithering gives the best visual fidelity.

### Ordered (Matrix) Dithering

**Ordered dithering** uses a fixed pattern (a _dither matrix_) to threshold pixels. It does not propagate error feedback like FS. Instead, for each pixel position `(x, y)`, a precomputed threshold offset (from a tileable matrix) is added to the pixel’s intensity to decide black or white. Common matrices include 8×8 or 4×4 Bayer matrices. This method produces a _regular dot pattern_ in the output. It is computationally cheap (no dependency between pixels, just a lookup and compare) and very predictable.

However, ordered dithering tends to sacrifice some detail and introduces a noticeable texture (the pattern of the matrix). As noted in an Adafruit guide, ordered patterns _“are usually not the best for photos, as they tend to lose edge details,”_ though they **can** provide a cleaner, more uniform look in areas of flat color or graphics. For example, large sky regions might look smoother (no random noise, just a gentle pattern), but fine details like hair or text overlaid on a photo might blur or disappear due to the threshold structure.

On a monochrome e-ink, ordered dithering can sometimes look less “noisy” to the eye than Floyd–Steinberg – the trade-off is that it may not render subtle shading as faithfully. Some users prefer the aesthetic of an ordered dither (it can look more like a deliberate stipple art or newspaper print). If the content is more iconographic (high-contrast graphics or UI elements) rather than a natural photograph, ordered dithering (or even no dithering) gives crisper results. In fact, if the image is already high contrast (like black text on white or simple logos), **no dithering** (`threshold` at 128 or a fixed level) is best to avoid any fuzziness.

For photos, if performance is extremely constrained, one might try a small matrix ordered dither (like 2×2 or 4×4) to reduce CPU usage. This will result in a coarser halftone look. Ordered dither is easy to implement in Rust: define a 2D array of thresholds (0–1 range or 0–255) and for each pixel do `new_val = (gray + matrix[y mod N][x mod N] > 128?)`. The matrix essentially ensures that, say, 25% gray pixels get one black pixel every 4 pixels in a fixed pattern.

**Blue-noise dithering:** A middle ground between error diffusion and ordered patterns is _blue noise_ masks (which are precomputed quasi-random threshold patterns that lack obvious repetition but don’t require per-pixel error calculation). These can be seen as a type of ordered dither with a large, irregular matrix. They can yield excellent subjective quality for static images and might be faster than full error diffusion. However, generating or storing a large blue-noise matrix (like 256×256) might be memory-heavy for embedded use, so it’s an option if memory allows.

In summary, **Floyd–Steinberg dithering is usually the top choice for photographic content on e-ink**, offering rich tonal reproduction, while **ordered dithering** (or pattern dithering) is an alternative when computational simplicity or a cleaner graphic style is needed. Many e-ink applications actually combine approaches: use error diffusion for images/photos, but use threshold or ordered dithering for UI elements or icons where you want crisp edges.

## Tonal Compression and Dynamic Range

As mentioned, mapping a photo’s wide tonal range to pure black and white is like squeezing a piano piece into two notes. Some form of **tonal compression** or mapping is necessary to ensure the result is visually pleasing. Dithering inherently does a form of tone mapping by distributing density of black pixels to approximate brightness. But we have control over how we prepare the tones _before_ dithering and even how we apply dithering.

**Global Tone Curve:** A common strategy is to apply a gamma or sigmoid tone curve to the grayscale image. For example, a slight gamma < 1 (like 0.8) will brighten the midtones and lift shadows, so that in the final dither more white pixels appear in those areas, preventing detail from disappearing into black. Conversely, one might darken highlights a tad so that not everything blows out to white. Essentially, you might **compress the dynamic range** of the image into a smaller range that the 1-bit output can approximate with patterns. Tools like Photoshop or GIMP allow adjusting “levels” or curves – in an automated pipeline you can achieve something similar by mapping the histogram of the image to utilize the full 0–255 range (ensuring the brightest significant pixel becomes white and darkest becomes black) and maybe applying a slight S-curve to boost contrast in mid-tones while protecting extremes.

**Dithering Diffusion Tuning:** The earlier discussion of diffusion amount is one direct way to handle dynamic range issues. By choosing a diffusion less than 100%, you are essentially saying: “Allow some areas to clip to pure white or pure black instead of faithfully reproducing every intensity difference.” This is a deliberate compression of tonal range. As the guide showed, _lower diffusion = more contrasty output_ (blowing out highlights) while _higher diffusion = smoother gradients but at the expense of looking “noisy”_. There is no one-size-fits-all setting; the _optimal value varies per image_. For instance, a portrait might benefit from slightly lower diffusion to keep skin highlights clean, whereas a landscape with lots of texture might tolerate higher diffusion to retain detail in clouds and shadows. If implementing this in code, one can multiply the calculated error by some factor <1 (e.g., 0.8) before distributing to neighbors. This effectively leaves some error undistributed, which accumulates as clipping to black/white in certain areas.

**Local Adaptation:** An advanced technique (often beyond the capabilities of small embedded systems) is _adaptive local contrast_: e.g., using algorithms like CLAHE (Contrast-Limited Adaptive Histogram Equalization) to locally boost contrast in faint areas so they don’t vanish in dithering. This can help, for example, a faint object on a bright background to be emphasized by locally increasing its contrast. However, CLAHE is computationally heavy. A simpler heuristic might be adjusting the threshold locally based on local average intensity (sometimes called adaptive dithering). These techniques can yield better detail but are complex to implement under tight constraints. They are not commonly used in straightforward e-ink pipelines due to their cost.

**Calibration to E-Ink:** Tonal compression should also consider the characteristics of the specific e-ink panel. For example, if the “white” of the panel is not very bright (perhaps \~40% reflectance), then an image that is dithered to an average of 50% black pixels might look darker than expected. One could compensate by biasing the image a bit brighter before dithering to account for the e-ink’s lower white point. Additionally, e-ink exhibits a slight **hysteresis** in contrast – small isolated black pixels on a mostly white background might appear lighter (or not fully formed) if a full refresh isn’t done. The safe approach is to always do a full refresh for photos and assume the panel will faithfully show black vs white. But in case of any systematic bias (say the panel’s black isn’t very dark), a calibration could be applied (e.g., treat 5% gray in source as black if the panel can’t differentiate anything lighter than that). In practice, community projects sometimes build **calibration tables** for e-ink displays to tweak how images are rendered for more “realistic photo display” on that specific hardware. This usually involves empirically determining a curve that maps source intensity to dot patterns given the display’s contrast response.

To summarize, handling tonal compression involves balancing **contrast vs detail**: we want the final image to have some true whites and blacks for punch, yet we want to preserve as many midtone details as possible. Global contrast adjustments and tuning the dithering algorithm’s behavior are effective tools. Always test the output on the actual e-ink – what looks good on a PC screen (which has a bright white background) may look different on paper-like e-ink. You might iterate on gamma/contrast until the e-ink photo “pops” without losing the essence of the image.

## Performance Considerations on TRMNL (Embedded Rust)

Running image processing on a microcontroller or embedded Linux in Rust requires attention to efficiency:

- **Algorithmic Complexity:** Floyd–Steinberg dithering is O(N) for N pixels, which is fine (e.g., \~384k pixels for 800×480). The main overhead is JPEG decoding, which is significantly more CPU-intensive. Using an optimized JPEG decoder with some hardware acceleration (SIMD) can help on platforms that support it. On microcontrollers without SIMD, ensure the decoder is built with optimized C or Rust code. The Rust crate _zune-jpeg_ aims to be fast, nearing libjpeg-turbo speeds on supported platforms. If decoding is too slow, consider reducing image size or using a simpler image format (like a pre-converted 1-bit BMP, though that shifts the burden outside the device).

- **Memory Usage:** A full 800×480 8-bit grayscale image is 384 KB. Some embedded systems might not comfortably hold that in RAM. If memory is constrained, consider streaming the image processing. For example, decode the JPEG a chunk at a time and immediately dither line by line. Floyd–Steinberg needs the next line of pixels for error distribution, so you could buffer two lines at a time instead of the whole image. The output 1-bit image is 800×480/8 ≈ 48 KB, which is much more manageable; you might allocate a frame buffer of this size and fill it progressively. The **embedded-graphics** Rust library demonstrates a philosophy of not needing large buffers at all – it draws pixels on the fly using iterators, requiring minimal RAM. A similar approach can be taken: fetch a pixel, compute output, send to display (or buffer) immediately.

- **no_std and Allocators:** In a pure no_std environment (no OS, using Rust core), you cannot use heap allocations unless you bring in an allocator. Many image libraries (e.g., `jpeg-decoder`) can work with `alloc` enabled (so you provide a global allocator). If your system has at least tens of KB of heap available, this is simplest. Otherwise, you may need to use `heapless` data structures or stack buffers. For instance, one could allocate a couple of line buffers statically for the decode and dithering process. Libraries like _embedded-graphics_ are designed to avoid heap entirely, working without any dynamic memory, which is ideal for tight systems.

- **Integer vs Floating-Point:** As mentioned, using integer math is preferable for performance. All dithering computations (diffusing fractions like 7/16) can be done with integer arithmetic (e.g., multiply by 7 and divide by 16, or use bit shifts if denominators are powers of two). Color to grayscale conversion can use integer math as well (e.g., `(299*R + 587*G + 114*B)/1000` for the Rec.601 luma). Avoiding floating point not only ensures compatibility with no-FPU microcontrollers, it also prevents pulling in the heavy `libm` math library into a no_std binary.

- **Lookup Tables:** If doing gamma correction or any non-linear mapping, precompute a lookup table (0–255 -> adjusted value) rather than computing exponents on the fly. A 256-byte table for gamma is trivial in memory and saves CPU cycles.

- **Parallelism:** On systems with multiple cores (some embedded boards or an ESP32 with two cores), you could split the image for processing. However, error diffusion is inherently serial (each pixel depends on the previous). You could potentially split by rows (e.g., even rows processed on one core, odd on another) if you handle error at the boundary carefully or accept a slight seam. This is likely unnecessary unless you have a beefy embedded CPU where threads make sense (e.g., on a Raspberry Pi class device, but then you’re less constrained anyway).

- **E-ink I/O and Updates:** The act of sending the processed image to the e-ink display can also be a bottleneck. Many e-ink controllers (like those on Waveshare hats) use SPI to receive the pixel data. Sending 48 KB over SPI can take some tens of milliseconds at high SPI clock (say 10 MHz), which is usually fine relative to the overall 1s refresh. Still, ensure your drawing routine is efficient (perhaps use DMA for SPI transfer if available). The good news is that since e-ink is slow to refresh, the processing time for dithering (which might be on the order of a few hundred milliseconds in C/Rust) can often be masked under the refresh time.

- **Battery and Performance Trade-off:** Performing image conversion on the device uses CPU and thus battery. If images do not change frequently, this is a one-time cost per update. If the device pulls a new photo only occasionally (like a few times a day), the impact on battery is minimal compared to the e-ink refresh itself and wireless communications. However, if one were trying to do many updates or even animations, a simpler algorithm (like ordered dither or precomputed frames) would be more power-friendly. In extreme cases, offloading the image processing to a server or host (and sending an already-dithered bitmap to the device) is a strategy – indeed, the TRMNL platform often expects the server to send ready-to-display 1-bit images. In a DIY scenario though, you might want the device to be self-contained, which is where careful optimization in Rust matters.

## Rust Implementation and Libraries (Embedded-Friendly)

To implement the above in Rust, we can leverage both **high-level libraries** (when running on a PC or a beefier device) and **embedded-specific crates** when on no_std microcontrollers:

- **JPEG Decoding:** As noted, _zune-jpeg_ is a pure Rust decoder that supports no_std (with the `alloc` feature). It has optimizations and is maintained for performance. Another option is _jpeg-decoder_ crate, which is also pure Rust; it might allocate a `Vec<u8>` for the output image but could potentially be adapted for no_std by providing a custom allocator. If memory allows, decoding to an `Image` buffer in RAM is simplest. On resource-limited targets, consider decoding to grayscale directly (some decoders let you skip the color conversion of JPEG and output an L8 grayscale image, saving time and RAM). Also consider whether you can reduce JPEG quality or resolution upstream to make decoding easier downstream.

- **Image Processing:** The Rust `image` crate (from the image-rs ecosystem) has functions for resizing, color space conversion, and even dithering (`imageops::dither` uses Floyd–Steinberg). However, `image` is a large dependency and not no_std (it requires std). It’s great for prototyping on a PC, but for embedded, it’s often better to write a small custom routine or use a lighter crate. _imageproc_ and _image-effects_ crates provide various filters and dithering as well, but similarly may not be no_std. In embedded, one typically writes a loop for scaling or uses integer arithmetic as described. If using embedded-graphics, you can draw a resized image by iterating over source pixels and plotting to target coordinates.

- **Embedded Graphics Ecosystem:** The **embedded-graphics** crate is highly relevant for drawing on displays. It provides a `DrawTarget` trait which many e-ink display drivers implement. For example, the **epd-waveshare** crate (if TRMNL’s display controller is supported there) can take a framebuffer or accept drawing commands. With embedded-graphics, you could avoid manual bit-banging of pixels: simply create a 1-bit image object and use `draw_pixel` or `.draw_iter()` to push it to the display driver. Notably, embedded-graphics is **no_std, no_alloc** and uses iterators so that you don’t need to hold huge buffers. There are helper crates like `tinybmp`, `tinytga`, etc., to load images in those formats within embedded-graphics. If you were to pre-dither a photo (e.g., offline) and store it as a BMP in flash, `tinybmp` could load it and display it. But if generating on the fly, you’ll be constructing it manually anyway.

- **Dithering Implementation:** If you prefer not to “reinvent the wheel” for dithering, you could look at crates like **dither** or **image-effects**, which implement Floyd–Steinberg and ordered dithers. These might not be no_std, but you could study their implementation (which will be similar to the pseudocode above) and adapt it. Given the simplicity of FS, many developers just implement it directly. The pseudocode we provided can be translated to Rust easily, taking care to handle edges (no neighbors outside bounds) and using perhaps `i16` or `i32` accumulators for error to avoid overflow. For ordered dithering, you can hardcode a small matrix, e.g.:

    ```rust
    let bayer2x2: [[u8; 2]; 2] = [[0, 128],
                                  [192, 64]];
    for y in 0..h {
        for x in 0..w {
            let val = gray[y][x];
            // Bayer matrix scaled to 0-255 range for threshold
            let threshold = bayer2x2[y % 2][x % 2];
            output[y][x] = if val > threshold { WHITE } else { BLACK };
        }
    }
    ```

    This example 2×2 matrix produces a simple pattern (at 50% gray it will alternate black/white pixels). Larger matrices (4×4, 8×8) produce finer patterns. You can find Bayer matrices or even blue noise matrices online to plug in.

- **Displaying the Image:** Once you have the 1-bit output (likely stored as a byte array where each byte holds 8 pixels for e-ink), use the display’s driver to send it. Many e-ink drivers expect a buffer of bytes for the full screen. If using a driver crate, it might expose a method like `display.draw(buffer)` or you may use embedded-graphics’ traits. For TRMNL specifically, if writing your own firmware, you’d interact with whatever controller (possibly an IL0373 or similar EPD controller) over SPI, toggling DC/CS pins as needed. There are open-source projects for similar 7.5″ e-inks you can reference (for example, the **GxEPD** library in Arduino world, or its Rust equivalents).

- **Testing and Iteration:** Because tuning is important, it’s wise to first test your image pipeline on a PC with the same parameters. You can write a small Rust program (with `image` crate) that decodes a JPEG, applies your dithering function, and saves a BMP/PNG. You can compare it to known good outputs (like ImageMagick’s `-monochrome`). This will validate your approach. Then, ensure your embedded implementation yields the same results. This two-step approach helps catch any discrepancies (for example, if your gamma handling is off, you’ll see the difference). Keep in mind the final arbiter is how it looks on _real e-ink_ – so you might need to adjust contrast or diffusion based on actual device tests, not just the simulated image.

**Rust Crate Recommendations Summary:** For JPEG decoding in no_std: _zune-jpeg_ is a top choice (fast and safe). For drawing and display: _embedded-graphics_ (with an appropriate display driver crate) is highly recommended to manage the framebuffer efficiently. If working with images on a system with std, the _image_ crate plus _imageops::dither_ (FloydSteinberg) can be used as reference or tooling. But in an embedded firmware, lightweight custom code is often the way to go for full control and minimal overhead.

## Conclusion

Transforming a JPEG photo for a 1-bit e-ink display like TRMNL’s involves a blend of **art and engineering**. We must artistically choose how to map grays to black or white (through contrast tweaks and dithering choices) while also engineering the solution to run on limited hardware (using efficient algorithms and Rust’s no_std capabilities). By preprocessing the image (resizing and adjusting tones), applying a well-suited dithering method (Floyd–Steinberg for most photos, with possible tweaks), and optimizing the implementation for performance, we can achieve surprisingly good photographic renderings on a monochrome e-ink screen. E-ink’s paper-like quality combined with proper dithering can produce a pleasing, almost newsprint-like photo. With Rust, we have the tools to implement this pipeline safely in embedded environments. The result is that even in a constrained setting, the TRMNL can faithfully display the essence of a photograph – in crisp blacks and whites – for a truly focused, distraction-free visual experience.

**Sources:** The techniques and recommendations above are informed by known practices in e-ink image processing and Rust embedded development, including guidance from Adafruit on preparing e-ink graphics, insights on dithering trade-offs, and documentation of Rust libraries suitable for no_std environments. These sources underline the importance of dithering for gradation on limited displays and provide real-world validation for the approaches discussed. By combining these strategies, developers can render photos on TRMNL’s display both accurately and efficiently.
